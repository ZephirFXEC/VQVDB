{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae509fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.VQVAE_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8192\n",
    "EPOCHS = 50\n",
    "LR = 5e-4\n",
    "IN_CHANNELS = 3\n",
    "EMBEDDING_DIM = 128  # The dimensionality of the embeddings\n",
    "NUM_EMBEDDINGS = 256  # The size of the codebook (the \"dictionary\")\n",
    "COMMITMENT_COST = 0.25\n",
    "\n",
    "device = \"cuda\"\n",
    "data_dir = \"C:/Users/zphrfx/Desktop/hdk/VQVDB/data/vdb_cache/npy\"\n",
    "\n",
    "\n",
    "model = VQVAE(\n",
    "    in_channels=IN_CHANNELS,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_embeddings=NUM_EMBEDDINGS,\n",
    "    commitment_cost=COMMITMENT_COST,\n",
    ").to(device)\n",
    "\n",
    "npy_files = list(Path(data_dir).glob(\"*.npy\"))\n",
    "if not npy_files:\n",
    "    raise ValueError(f\"No .npy files found in /data/npy\")\n",
    "\n",
    "print(f\"Found {len(npy_files)} .npy files\")\n",
    "\n",
    "vdb_dataset = VDBLeafDataset(npy_files=npy_files, include_origins=False, in_channels=IN_CHANNELS)\n",
    "\n",
    "# Save the model state_dict\n",
    "model_path = \"C:/Users/zphrfx/Desktop/hdk/VQVDB/python/models/vqvae.pth\"\n",
    "\n",
    "# Visualize the reconstruction quality for a single example\n",
    "save = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(save[\"state_dict\"])\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Reconstruction Quality for a Single Example\")\n",
    "\n",
    "\n",
    "original_block = vdb_dataset[78].unsqueeze(0).to(device)\n",
    "\n",
    "print(\"Performing reconstruction...\")\n",
    "with torch.no_grad():\n",
    "    indices = model.encode(original_block)\n",
    "    reconstructed_block = model.decode(indices)\n",
    "\n",
    "# --- 4. Prepare Data for Plotting ---\n",
    "# Convert to NumPy and permute from (C, D, H, W) to (D, H, W, C) for easier slicing\n",
    "original_np = original_block.squeeze(0).permute(1, 2, 3, 0).cpu().numpy()\n",
    "reconstructed_np = reconstructed_block.squeeze(0).permute(1, 2, 3, 0).detach().cpu().numpy()\n",
    "\n",
    "# Calculate the magnitude of the error vector for heatmap visualization\n",
    "error_magnitude_np = np.linalg.norm(original_np - reconstructed_np, axis=-1)\n",
    "\n",
    "# Helper function to map vector components from [-1, 1] to a [0, 1] RGB image\n",
    "def vector_to_rgb(vector_slice):\n",
    "    # Data is in [-1, 1], so shift and scale to [0, 1] for RGB display\n",
    "    return np.clip((vector_slice * 0.5) + 0.5, 0, 1)\n",
    "\n",
    "# --- 5. Plotting ---\n",
    "print(\"Generating plots...\")\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10), constrained_layout=True)\n",
    "center_slice_idx = 4  # Center slice for visualization\n",
    "\n",
    "# --- Row 1: Z-Axis Slice ---\n",
    "axes[0, 0].imshow(vector_to_rgb(original_np[center_slice_idx, :, :, :]))\n",
    "axes[0, 0].set_title(f'Original (Slice Z={center_slice_idx})')\n",
    "\n",
    "im_err_z = axes[0, 2].imshow(error_magnitude_np[center_slice_idx, :, :], cmap='magma')\n",
    "axes[0, 2].set_title('Error Magnitude')\n",
    "\n",
    "axes[0, 1].imshow(vector_to_rgb(reconstructed_np[center_slice_idx, :, :, :]))\n",
    "axes[0, 1].set_title(f'Reconstructed (Slice Z={center_slice_idx})')\n",
    "\n",
    "# --- Row 2: Y-Axis Slice ---\n",
    "axes[1, 0].imshow(vector_to_rgb(original_np[:, center_slice_idx, :, :]))\n",
    "axes[1, 0].set_title(f'Original (Slice Y={center_slice_idx})')\n",
    "\n",
    "im_err_y = axes[1, 2].imshow(error_magnitude_np[:, center_slice_idx, :], cmap='magma', vmin=im_err_z.get_clim()[0], vmax=im_err_z.get_clim()[1])\n",
    "axes[1, 2].set_title('Error Magnitude')\n",
    "\n",
    "axes[1, 1].imshow(vector_to_rgb(reconstructed_np[:, center_slice_idx, :, :]))\n",
    "axes[1, 1].set_title(f'Reconstructed (Slice Y={center_slice_idx})')\n",
    "\n",
    "# --- Row 3: X-Axis Slice ---\n",
    "axes[2, 0].imshow(vector_to_rgb(original_np[:, :, center_slice_idx, :]))\n",
    "axes[2, 0].set_title(f'Original (Slice X={center_slice_idx})')\n",
    "\n",
    "im_err_x = axes[2, 2].imshow(error_magnitude_np[:, :, center_slice_idx], cmap='magma', vmin=im_err_z.get_clim()[0], vmax=im_err_z.get_clim()[1])\n",
    "axes[2, 2].set_title('Error Magnitude')\n",
    "\n",
    "axes[2, 1].imshow(vector_to_rgb(reconstructed_np[:, :, center_slice_idx, :]))\n",
    "axes[2, 1].set_title(f'Reconstructed (Slice X={center_slice_idx})')\n",
    "\n",
    "# --- Final Touches ---\n",
    "# Turn off all axes\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add a single, shared colorbar for the error magnitude plots\n",
    "fig.colorbar(im_err_z, ax=axes[:, 2], orientation='vertical', label='Error Vector Magnitude', shrink=0.8)\n",
    "\n",
    "plt.suptitle('Vector Field Reconstruction (XYZ -> RGB)', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot losses and perplexity\n",
    "rl = save['recon_loss_l']\n",
    "vq_loss_l = save['vq_loss_l']\n",
    "perplexity_l = save['perplexity_l']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(rl, label='Reconstruction Loss', color='blue')\n",
    "plt.plot(vq_loss_l, label='VQ Loss', color='orange')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(perplexity_l, label='Perplexity', color='green')\n",
    "plt.title('Perplexity Over Training Steps')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_np.flatten(), bins=50, alpha=0.7, label='Original')\n",
    "plt.hist(reconstructed_np.flatten(), bins=50, alpha=0.7, label='Reconstructed')\n",
    "plt.title('Histogram of Voxel Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import entropy\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence between two distributions.\"\"\"\n",
    "    p = p.flatten()\n",
    "    q = q.flatten()\n",
    "    p = p / np.sum(p)  # Normalize\n",
    "    q = q / np.sum(q)  # Normalize\n",
    "    return entropy(p, q)\n",
    "kl_div = kl_divergence(original_np, reconstructed_np)\n",
    "print(f\"KL Divergence between original and reconstructed blocks: {kl_div:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA of the learned codebook vectors:\")\n",
    "codebook = model.quantizer.embedding.data.cpu()\n",
    "pca = PCA(n_components=2)\n",
    "codebook_2d = pca.fit_transform(codebook)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], s=15, alpha=0.7)\n",
    "plt.title('VQ-VAE Codebook (PCA Projection)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Codebook Usage Histogram ---\n",
    "# This is a powerful diagnostic. It requires running the encoder on the whole dataset.\n",
    "print(\"\\nCalculating codebook usage across the entire dataset...\")\n",
    "model.eval()\n",
    "all_indices = []\n",
    "# Create a dataloader without shuffling to iterate through the dataset\n",
    "full_loader = DataLoader(vdb_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_batch in full_loader:\n",
    "        data_batch = data_batch.to(device) # Move data to the same device as the model\n",
    "        indices = model.encode(data_batch)\n",
    "        all_indices.append(indices.cpu().numpy().flatten())\n",
    "\n",
    "all_indices = np.concatenate(all_indices)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_indices, bins=NUM_EMBEDDINGS, range=(0, NUM_EMBEDDINGS-1))\n",
    "plt.title('Codebook Usage Frequency')\n",
    "plt.xlabel('Codebook Index')\n",
    "plt.ylabel('Number of Times Used')\n",
    "plt.show()\n",
    "\n",
    "num_dead_codes = NUM_EMBEDDINGS - len(np.unique(all_indices))\n",
    "print(f\"Number of 'dead' (unused) codes: {num_dead_codes} out of {NUM_EMBEDDINGS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb13bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Codebook Perplexity + Active-Code Ratio ---\n",
    "counts = np.bincount(all_indices, minlength=NUM_EMBEDDINGS).astype(np.float64)\n",
    "probs = counts / counts.sum()\n",
    "nonzero = probs > 0\n",
    "perplexity = np.exp(-(probs[nonzero] * np.log(probs[nonzero])).sum())\n",
    "active_ratio = nonzero.mean()\n",
    "\n",
    "print(f\"Codebook perplexity: {perplexity:.2f}\")\n",
    "print(f\"Active-code ratio  : {active_ratio*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def psnr(x, y, vmax=1.0):\n",
    "    mse = torch.mean((x - y) ** 2).item()\n",
    "    return 20 * log10(vmax) - 10 * log10(mse + 1e-12)\n",
    "\n",
    "model.eval()\n",
    "psnr_list, mse_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(vdb_dataset, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        batch = batch.to(device)\n",
    "        rec = model.decode(model.encode(batch))\n",
    "        mse = ((batch - rec) ** 2).view(len(batch), -1).mean(dim=1)\n",
    "        mse_list.extend(mse.cpu().numpy())\n",
    "        psnr_list.extend([psnr(b, r) for b, r in zip(batch, rec)])\n",
    "\n",
    "avg_psnr = np.mean(psnr_list)\n",
    "avg_mse = np.mean(mse_list)\n",
    "\n",
    "# Create publication-ready plots\n",
    "plt.style.use('default')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# PSNR Distribution\n",
    "ax1.hist(psnr_list, bins=40, alpha=0.7, color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "ax1.axvline(avg_psnr, color='crimson', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {avg_psnr:.1f} dB')\n",
    "ax1.set_xlabel('PSNR (dB)', fontsize=12)\n",
    "ax1.set_ylabel('Number of Blocks', fontsize=12)\n",
    "ax1.set_title('PSNR Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(labelsize=10)\n",
    "\n",
    "# MSE Distribution\n",
    "ax2.hist(mse_list, bins=40, alpha=0.7, color='forestgreen', edgecolor='black', linewidth=0.5)\n",
    "ax2.axvline(avg_mse, color='crimson', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {avg_mse:.2e}')\n",
    "ax2.set_xlabel('MSE', fontsize=12)\n",
    "ax2.set_ylabel('Number of Blocks (log scale)', fontsize=12)\n",
    "ax2.set_title('MSE Distribution', fontsize=13, fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Reconstruction Quality Metrics:\")\n",
    "print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "print(f\"Average MSE: {avg_mse:.2e}\")\n",
    "print(f\"PSNR std: {np.std(psnr_list):.2f} dB\")\n",
    "print(f\"MSE std: {np.std(mse_list):.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f19461",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100000\n",
    "orig_sample = original_np.flatten()\n",
    "recon_sample = reconstructed_np.flatten()\n",
    "if len(orig_sample) > n_points:\n",
    "    idx = np.random.choice(len(orig_sample), n_points, replace=False)\n",
    "    orig_sample = orig_sample[idx]; recon_sample = recon_sample[idx]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(orig_sample, recon_sample, s=2, alpha=.5)\n",
    "lims = [min(orig_sample.min(), recon_sample.min()),\n",
    "        max(orig_sample.max(), recon_sample.max())]\n",
    "plt.plot(lims, lims, 'k--', linewidth=1)\n",
    "plt.xlabel('Original voxel'); plt.ylabel('Reconstructed voxel')\n",
    "plt.title('Voxel-wise Scatter (diag = perfect)')\n",
    "plt.grid(True, alpha=.3); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20837d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- L2 norm of each embedding vector ---\n",
    "embed_norm = torch.linalg.norm(model.quantizer.embedding.data, dim=1).cpu().numpy()\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.bar(range(NUM_EMBEDDINGS), embed_norm, width=1.0)\n",
    "plt.title('Codebook Embedding L2 Norms'); plt.xlabel('Code Index'); plt.ylabel('Norm')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f22a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mip(vol, axis):\n",
    "    \"\"\"Maximum-intensity projection along a single axis.\"\"\"\n",
    "    return vol.max(axis=axis)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 7))\n",
    "views = [(0, 'XY MIP'),   # collapse Z\n",
    "         (1, 'XZ MIP'),   # collapse Y\n",
    "         (2, 'YZ MIP')]   # collapse X\n",
    "\n",
    "for col, (axis_to_collapse, title) in enumerate(views):\n",
    "    axes[0, col].imshow(mip(vector_to_rgb(original_np), axis=axis_to_collapse), cmap='viridis')\n",
    "    axes[0, col].set_title(f'Original {title}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(mip(vector_to_rgb(reconstructed_np), axis=axis_to_collapse), cmap='viridis')\n",
    "    axes[1, col].set_title(f'Reconstructed {title}')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Maximum-Intensity Projections (3-view)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c617293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. Build a per-block latent vector ----------\n",
    "model.eval()\n",
    "latents, errs = [], []          # errs = optional colouring\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(vdb_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        idx = model.encode(batch).long()               # (B, Z, Y, X) indices\n",
    "        emb = model.quantizer.embedding[idx.view(-1)]  # (B*Z*Y*X, C)\n",
    "        emb = emb.view(*idx.shape, -1)                 # (B, Z, Y, X, C)\n",
    "        mean_emb = emb.mean(dim=(1, 2, 3))             # (B, C)\n",
    "        latents.append(mean_emb.cpu())\n",
    "        \n",
    "        # Optional: per-block MSE for coloured scatter\n",
    "        rec = model.decode(idx)\n",
    "        errs.append(((batch - rec) ** 2)\n",
    "                    .view(len(batch), -1)\n",
    "                    .mean(dim=1)\n",
    "                    .cpu())\n",
    "\n",
    "latents = torch.cat(latents, dim=0).numpy()   # (N, C)\n",
    "errs    = torch.cat(errs, dim=0).numpy()      # (N,)\n",
    "\n",
    "# ---------- 2. PCA to 2-D ----------\n",
    "pca2 = FastICA(n_components=2, random_state=0)\n",
    "latents_2d = pca2.fit_transform(latents)      # (N, 2)\n",
    "\n",
    "\n",
    "sc = plt.scatter(latents_2d[:, 0],\n",
    "                 latents_2d[:, 1],\n",
    "                 c=errs,                 # <- set to None for uniform colour\n",
    "                 cmap='viridis',\n",
    "                 s=4,\n",
    "                 alpha=0.8)\n",
    "if sc.get_array() is not None:           # only if colouring by a value\n",
    "    plt.colorbar(sc, label='Block MSE')\n",
    "\n",
    "plt.title('Latent Space Sampling (PCA-2D, viridis)')\n",
    "plt.xlabel('PC-1'); plt.ylabel('PC-2')\n",
    "plt.grid(True, alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
