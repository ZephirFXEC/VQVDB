{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDBLeafDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            npy_files: Sequence[str | Path],\n",
    "            transform: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "            *,\n",
    "            include_origins: bool = False,\n",
    "            origins_root: str | Path | None = None,\n",
    "            origins_suffix: str = \"._origins.npy\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.include_origins = include_origins\n",
    "\n",
    "        # --- mmap all data files -------------------------------------------------\n",
    "        self.arrays: List[np.memmap] = []\n",
    "        self.origin_arrays: List[np.memmap] | None = [] if include_origins else None\n",
    "\n",
    "        for f in npy_files:\n",
    "            arr = np.load(f, mmap_mode=\"r\")\n",
    "            if arr.shape[1:] != (8, 8, 8):\n",
    "                raise ValueError(f\"{f}: expected (N, 8, 8, 8), got {arr.shape}\")\n",
    "            self.arrays.append(arr)\n",
    "\n",
    "            if include_origins:\n",
    "                if origins_root is not None:\n",
    "                    origin_path = Path(origins_root) / (Path(f).stem + origins_suffix)\n",
    "                else:\n",
    "                    origin_path = Path(f).with_suffix(origins_suffix)\n",
    "                if not origin_path.exists():\n",
    "                    raise FileNotFoundError(origin_path)\n",
    "\n",
    "                self.origin_arrays.append(np.load(origin_path, mmap_mode=\"r\"))\n",
    "\n",
    "        # --- pre-compute global index mapping ------------------------------------\n",
    "        lengths = np.fromiter((a.shape[0] for a in self.arrays), dtype=np.int64)\n",
    "        self.file_offsets = np.concatenate(([0], np.cumsum(lengths)))\n",
    "        self.total_leaves: int = int(self.file_offsets[-1])\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_leaves\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if not (0 <= idx < self.total_leaves):\n",
    "            raise IndexError(idx)\n",
    "\n",
    "        # locate (file, local) in O(log n) inside highly-optimised C code\n",
    "        file_idx = int(np.searchsorted(self.file_offsets, idx, side=\"right\") - 1)\n",
    "        local_idx = idx - int(self.file_offsets[file_idx])\n",
    "\n",
    "        # zero-copy view from the mmap’d array\n",
    "        leaf_np = self.arrays[file_idx][local_idx].astype(np.float32, copy=True)\n",
    "        leaf = torch.from_numpy(leaf_np).to(torch.float32).unsqueeze(0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            leaf = self.transform(leaf)\n",
    "\n",
    "        if self.include_origins:\n",
    "            origin_np = self.origin_arrays[file_idx][local_idx].astype(np.int32, copy=False)\n",
    "            origin = torch.from_numpy(origin_np)\n",
    "            return leaf, origin\n",
    "\n",
    "        return leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int,\n",
    "                 commitment_cost: float, decay: float = 0.99, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        # Initialize embeddings\n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        embed = F.normalize(embed, dim=1)  # Normalize initial embeddings\n",
    "        \n",
    "        self.register_buffer('embedding', embed)\n",
    "        self.register_buffer('cluster_size', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('embed_avg', embed.clone().detach())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, *spatial = x.shape\n",
    "        flat = x.permute(0, *range(2, 2+len(spatial)), 1).contiguous().view(-1, D)\n",
    "\n",
    "        # Compute distances\n",
    "        distances = (\n",
    "            flat.pow(2).sum(1, keepdim=True)\n",
    "            + self.embedding.pow(2).sum(1)\n",
    "            - 2 * flat @ self.embedding.t()\n",
    "        )\n",
    "\n",
    "        # Get nearest codes\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        encodings = F.one_hot(encoding_indices, self.num_embeddings).type(flat.dtype)\n",
    "\n",
    "        # Quantize\n",
    "        quantized = encodings @ self.embedding\n",
    "        quantized = quantized.view(B, *spatial, D).permute(0, -1, *range(1, 1+len(spatial)))\n",
    "\n",
    "        # EMA updates (simplified)\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                encodings_sum = encodings.sum(0)\n",
    "                self.cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1-self.decay)\n",
    "                \n",
    "                dw = encodings.t() @ flat.detach()\n",
    "                self.embed_avg.mul_(self.decay).add_(dw, alpha=1-self.decay)\n",
    "                \n",
    "                # Normalize\n",
    "                n = self.cluster_size + self.eps\n",
    "                self.embedding.copy_(self.embed_avg / n.unsqueeze(1))\n",
    "\n",
    "        # Losses\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        # Perplexity\n",
    "        avg_probs = encodings.mean(0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, loss, perplexity\n",
    "\n",
    "\n",
    "# --- Encoder ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv3d(32, embedding_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Decoder ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # 4³ → 8³\n",
    "            nn.ConvTranspose3d(embedding_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # refine at 8³\n",
    "            nn.Conv3d(64, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Full VQ-VAE Model ---\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim, num_embeddings, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, embedding_dim)\n",
    "        self.quantizer = VectorQuantizerEMA(num_embeddings, embedding_dim, commitment_cost=0.25)\n",
    "        self.decoder = Decoder(embedding_dim, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, perplexity = self.quantizer(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss, perplexity\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        flat_z = z.permute(0, *range(2, z.ndim), 1).contiguous().view(-1, self.quantizer.embedding_dim)\n",
    "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) \n",
    "                     + torch.sum(self.quantizer.embedding**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_z, self.quantizer.embedding.t()))\n",
    "        indices = torch.argmin(distances, dim=1)\n",
    "        return indices.view(z.shape[0], *z.shape[2:])\n",
    "\n",
    "    def decode(self, indices):\n",
    "        quantized_vectors = F.embedding(indices, self.quantizer.embedding)\n",
    "        quantized_for_decoder = quantized_vectors.permute(0, quantized_vectors.ndim - 1, *range(1, quantized_vectors.ndim - 1))\n",
    "        x_recon = self.decoder(quantized_for_decoder)\n",
    "        return x_recon\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 8192\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "IN_CHANNELS = 1\n",
    "EMBEDDING_DIM = 64 # The dimensionality of the embeddings\n",
    "NUM_EMBEDDINGS = 512 # The size of the codebook (the \"dictionary\")\n",
    "COMMITMENT_COST = 0.25\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "npy_files = list(Path(\"data/npy\").glob(\"*.npy\"))\n",
    "if not npy_files:\n",
    "    raise ValueError(f\"No .npy files found in /data/npy\")\n",
    "\n",
    "print(f\"Found {len(npy_files)} .npy files\")\n",
    "\n",
    "vdb_dataset = VDBLeafDataset(npy_files=npy_files, include_origins=False)\n",
    "print(f\"Dataset created with {len(vdb_dataset)} total blocks.\")\n",
    "\n",
    "# keep 10% of the dataset for validation\n",
    "split_idx = int(len(vdb_dataset) * 0.1)\n",
    "vdb_dataset_train = torch.utils.data.Subset(vdb_dataset, range(split_idx))\n",
    "vdb_dataset_val = torch.utils.data.Subset(vdb_dataset, range(split_idx, len(vdb_dataset)))\n",
    "print(f\"Training dataset size: {len(vdb_dataset_train)}\")\n",
    "print(f\"Validation dataset size: {len(vdb_dataset_val)}\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    vdb_dataset_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VQVAE(IN_CHANNELS, EMBEDDING_DIM, NUM_EMBEDDINGS, COMMITMENT_COST).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "print(\"Starting training with data from DataLoader...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_recon_loss = 0.0\n",
    "    total_vq_loss = 0\n",
    "    \n",
    "    for i, data_batch in enumerate(train_loader):\n",
    "        leaves = data_batch.to(device, non_blocking=True)\n",
    "        \n",
    "        x_recon, vq_loss, perplexity = model(leaves)\n",
    "        recon_error = F.mse_loss(x_recon, leaves)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_recon_loss += recon_error.item()\n",
    "        total_vq_loss += vq_loss.item()\n",
    "\n",
    "    # Log progress at the end of each epoch\n",
    "    avg_recon_loss = total_recon_loss / len(train_loader)\n",
    "    avg_vq_loss = total_vq_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "            f\"Avg Recon Loss: {avg_recon_loss:.5f} | \"\n",
    "            f\"Avg VQ Loss: {avg_vq_loss:.5f} | \"\n",
    "            f\"Last Perplexity: {perplexity.item():.4f}\") # Perplexity from last batch\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Reconstruction Quality for a Single Example\")\n",
    "model.eval()\n",
    "\n",
    "# Get a random block from the dataset\n",
    "original_block = vdb_dataset[76].unsqueeze(0).to(device)\n",
    "\n",
    "# Perform the full compression/decompression cycle\n",
    "indices = model.encode(original_block)\n",
    "reconstructed_block = model.decode(indices)\n",
    "\n",
    "# Detach from GPU and convert to numpy for plotting\n",
    "original_np = original_block.squeeze().cpu().numpy()\n",
    "reconstructed_np = reconstructed_block.squeeze().detach().cpu().numpy()\n",
    "error_np = np.abs(original_np - reconstructed_np)\n",
    "\n",
    "# Get consistent color limits for fair comparison\n",
    "vmin = min(original_np.min(), reconstructed_np.min())\n",
    "vmax = max(original_np.max(), reconstructed_np.max())\n",
    "\n",
    "# --- Plot 1: Slice-by-Slice Comparison ---\n",
    "fig, axes = plt.subplots(3, 3, figsize=(13, 10))\n",
    "slices_to_show = [1, 4, 7] # Show slices from the Z-axis of the 8x8x8 cube\n",
    "\n",
    "for i, slice_idx in enumerate(slices_to_show):\n",
    "    # Original\n",
    "    im1 = axes[i, 0].imshow(original_np[slice_idx, :, :], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    axes[i, 0].set_title(f'Original (Slice Z={slice_idx})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Reconstructed\n",
    "    im2 = axes[i, 1].imshow(reconstructed_np[slice_idx, :, :], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    axes[i, 1].set_title(f'Reconstructed (Slice Z={slice_idx})')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Error Map\n",
    "    im3 = axes[i, 2].imshow(error_np[slice_idx, :, :], cmap='magma')\n",
    "    axes[i, 2].set_title('Absolute Error')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "fig.colorbar(im1, ax=axes[:,:2], orientation='vertical', fraction=.1)\n",
    "fig.colorbar(im3, ax=axes[:,2], orientation='vertical', fraction=.1)\n",
    "plt.suptitle('Qualitative Reconstruction Analysis', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_np.flatten(), bins=50, alpha=0.7, label='Original')\n",
    "plt.hist(reconstructed_np.flatten(), bins=50, alpha=0.7, label='Reconstructed')\n",
    "plt.title('Histogram of Voxel Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA of the learned codebook vectors:\")\n",
    "codebook = model.quantizer.embedding.data.cpu()\n",
    "pca = PCA(n_components=2)\n",
    "codebook_2d = pca.fit_transform(codebook)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], s=15, alpha=0.7)\n",
    "plt.title('VQ-VAE Codebook (PCA Projection)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Codebook Usage Histogram ---\n",
    "# This is a powerful diagnostic. It requires running the encoder on the whole dataset.\n",
    "print(\"\\nCalculating codebook usage across the entire dataset...\")\n",
    "model.eval()\n",
    "all_indices = []\n",
    "# Create a dataloader without shuffling to iterate through the dataset\n",
    "full_loader = DataLoader(vdb_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_batch in full_loader:\n",
    "        data_batch = data_batch.to(device) # Move data to the same device as the model\n",
    "        indices = model.encode(data_batch)\n",
    "        all_indices.append(indices.cpu().numpy().flatten())\n",
    "\n",
    "all_indices = np.concatenate(all_indices)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_indices, bins=NUM_EMBEDDINGS, range=(0, NUM_EMBEDDINGS-1))\n",
    "plt.title('Codebook Usage Frequency')\n",
    "plt.xlabel('Codebook Index')\n",
    "plt.ylabel('Number of Times Used')\n",
    "plt.show()\n",
    "\n",
    "num_dead_codes = NUM_EMBEDDINGS - len(np.unique(all_indices))\n",
    "print(f\"Number of 'dead' (unused) codes: {num_dead_codes} out of {NUM_EMBEDDINGS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
