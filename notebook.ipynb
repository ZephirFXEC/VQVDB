{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Callable, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# For visualization\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDBLeafDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            npy_files: Sequence[str | Path],\n",
    "            transform: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,\n",
    "            *,\n",
    "            include_origins: bool = False,\n",
    "            origins_root: str | Path | None = None,\n",
    "            origins_suffix: str = \"._origins.npy\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.include_origins = include_origins\n",
    "\n",
    "        # --- mmap all data files -------------------------------------------------\n",
    "        self.arrays: List[np.memmap] = []\n",
    "        self.origin_arrays: List[np.memmap] | None = [] if include_origins else None\n",
    "\n",
    "        for f in npy_files:\n",
    "            arr = np.load(f, mmap_mode=\"r\")\n",
    "            if arr.shape[1:] != (8, 8, 8):\n",
    "                raise ValueError(f\"{f}: expected (N, 8, 8, 8), got {arr.shape}\")\n",
    "            self.arrays.append(arr)\n",
    "\n",
    "            if include_origins:\n",
    "                if origins_root is not None:\n",
    "                    origin_path = Path(origins_root) / (Path(f).stem + origins_suffix)\n",
    "                else:\n",
    "                    origin_path = Path(f).with_suffix(origins_suffix)\n",
    "                if not origin_path.exists():\n",
    "                    raise FileNotFoundError(origin_path)\n",
    "\n",
    "                self.origin_arrays.append(np.load(origin_path, mmap_mode=\"r\"))\n",
    "\n",
    "        # --- pre-compute global index mapping ------------------------------------\n",
    "        lengths = np.fromiter((a.shape[0] for a in self.arrays), dtype=np.int64)\n",
    "        self.file_offsets = np.concatenate(([0], np.cumsum(lengths)))\n",
    "        self.total_leaves: int = int(self.file_offsets[-1])\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_leaves\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if not (0 <= idx < self.total_leaves):\n",
    "            raise IndexError(idx)\n",
    "\n",
    "        # locate (file, local) in O(log n) inside highly-optimised C code\n",
    "        file_idx = int(np.searchsorted(self.file_offsets, idx, side=\"right\") - 1)\n",
    "        local_idx = idx - int(self.file_offsets[file_idx])\n",
    "\n",
    "        # zero-copy view from the mmap’d array\n",
    "        leaf_np = self.arrays[file_idx][local_idx].astype(np.float32, copy=True)\n",
    "        leaf = torch.from_numpy(leaf_np).to(torch.float32).unsqueeze(0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            leaf = self.transform(leaf)\n",
    "\n",
    "        if self.include_origins:\n",
    "            origin_np = self.origin_arrays[file_idx][local_idx].astype(np.int32, copy=False)\n",
    "            origin = torch.from_numpy(origin_np)\n",
    "            return leaf, origin\n",
    "\n",
    "        return leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int,\n",
    "                 commitment_cost: float, decay: float = 0.99, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        # Initialize embeddings (small variance + normalize)\n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        embed = F.normalize(embed, dim=1)\n",
    "        \n",
    "        self.register_buffer('embedding', embed)\n",
    "        self.register_buffer('cluster_size', torch.ones(num_embeddings))\n",
    "        self.register_buffer('embed_avg', embed.clone().detach())\n",
    "\n",
    "    def forward(self, x)  -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        D = self.embedding_dim\n",
    "\n",
    "        # Build the permutation list explicitly: [0, 2, 3, …, n, 1]\n",
    "        permute_fwd: List[int] = [0] + list(range(2, x.dim())) + [1]\n",
    "        \n",
    "        permuted_x = torch.permute(x, permute_fwd).contiguous()\n",
    "        flat = permuted_x.view(-1, D)\n",
    "\n",
    "        # Compute distances\n",
    "        distances = (\n",
    "            torch.sum(flat**2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding**2, dim=1)\n",
    "            - 2 * torch.mm(flat, self.embedding.t())\n",
    "        )\n",
    "\n",
    "        # Get nearest codes\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        encodings = F.one_hot(encoding_indices, self.num_embeddings).type(flat.dtype)\n",
    "\n",
    "        # Quantize\n",
    "        quantized = encodings @ self.embedding\n",
    "        quantized = quantized.view(permuted_x.shape)\n",
    "        \n",
    "        permute_back: List[int] = [0, x.dim() - 1] + list(range(1, x.dim() - 1))\n",
    "        quantized = torch.permute(quantized, permute_back)\n",
    "\n",
    "        # EMA updates\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                encodings_sum = encodings.sum(0)\n",
    "                self.cluster_size.mul_(self.decay).add_(encodings_sum, alpha=1 - self.decay)\n",
    "                \n",
    "                dw = encodings.t() @ flat.detach()\n",
    "                self.embed_avg.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "                \n",
    "                n = self.cluster_size.clamp(min=self.eps)\n",
    "                self.embedding.copy_(self.embed_avg / n.unsqueeze(1))\n",
    "\n",
    "        commitment_loss = self.commitment_cost * F.mse_loss(x, quantized.detach())\n",
    "        loss = commitment_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        # Perplexity\n",
    "        avg_probs = encodings.mean(0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, loss, perplexity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 8³ → 4³\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Refine at 4³\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final projection\n",
    "            nn.Conv3d(64, embedding_dim, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Expand from embedding_dim\n",
    "            nn.Conv3d(embedding_dim, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 4³ → 8³\n",
    "            nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final reconstruction\n",
    "            nn.Conv3d(32, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim, num_embeddings, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, embedding_dim)\n",
    "        self.quantizer = VectorQuantizerEMA(num_embeddings, embedding_dim, commitment_cost=0.25)\n",
    "        self.decoder = Decoder(embedding_dim, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, perplexity = self.quantizer(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss, perplexity\n",
    "\n",
    "    def encode(self, x) -> torch.Tensor:\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        shape = list(z.shape)         # List[int]\n",
    "        B, D = shape[0], shape[1]\n",
    "        spatial = shape[2:]\n",
    "\n",
    "        permute_fwd = [0] + list(range(2, z.dim())) + [1]\n",
    "        flat_z = (torch.permute(z, permute_fwd)\n",
    "                  .contiguous()\n",
    "                  .view(-1, D)\n",
    "                  )\n",
    "    \n",
    "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) \n",
    "                     + torch.sum(self.quantizer.embedding**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_z, self.quantizer.embedding.t()))\n",
    "        indices = torch.argmin(distances, dim=1)\n",
    "        \n",
    "        return indices.view(B, *spatial)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def decode(self, indices):\n",
    "        quantized_vectors = F.embedding(indices, self.quantizer.embedding)\n",
    "        permute_back = [0, quantized_vectors.dim() - 1] + list(range(1, quantized_vectors.dim() - 1))\n",
    "        quantized_for_decoder = torch.permute(quantized_vectors, permute_back)\n",
    "        x_recon = self.decoder(quantized_for_decoder)\n",
    "        return x_recon\n",
    "    \n",
    "    def get_codebook(self) -> torch.Tensor:\n",
    "        return self.quantizer.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 8192*2\n",
    "EPOCHS = 15\n",
    "LR = 5e-4\n",
    "IN_CHANNELS = 1\n",
    "EMBEDDING_DIM = 128 # The dimensionality of the embeddings\n",
    "NUM_EMBEDDINGS = 256 # The size of the codebook (the \"dictionary\")\n",
    "COMMITMENT_COST = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "npy_files = list(Path(\"data/npy\").glob(\"*.npy\"))\n",
    "if not npy_files:\n",
    "    raise ValueError(f\"No .npy files found in /data/npy\")\n",
    "\n",
    "print(f\"Found {len(npy_files)} .npy files\")\n",
    "\n",
    "vdb_dataset = VDBLeafDataset(npy_files=npy_files, include_origins=False)\n",
    "print(f\"Dataset created with {len(vdb_dataset)} total blocks.\")\n",
    "\n",
    "# keep 10% of the dataset for validation\n",
    "split_idx = int(len(vdb_dataset) * 0.5)\n",
    "vdb_dataset_train = torch.utils.data.Subset(vdb_dataset, range(split_idx))\n",
    "vdb_dataset_val = torch.utils.data.Subset(vdb_dataset, range(split_idx, len(vdb_dataset)))\n",
    "print(f\"Training dataset size: {len(vdb_dataset_train)}\")\n",
    "print(f\"Validation dataset size: {len(vdb_dataset_val)}\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    vdb_dataset_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    vdb_dataset_val, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VQVAE(IN_CHANNELS, EMBEDDING_DIM, NUM_EMBEDDINGS, COMMITMENT_COST).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting training with data from DataLoader...\")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_recon_loss = 0.0\n",
    "    total_vq_loss = 0\n",
    "\n",
    "    for leaves in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        leaves = leaves.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, vq_loss, perplexity = model(leaves)\n",
    "        recon_error = F.mse_loss(x_recon, leaves)\n",
    "        loss = recon_error + vq_loss\n",
    "\n",
    "        loss.backward()\n",
    "        # Gradient clipping to stabilize training\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_recon_loss += recon_error.item()\n",
    "        total_vq_loss += vq_loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for leaves in val_loader:\n",
    "            leaves = leaves.to(device)\n",
    "            x_recon, vq_loss, _ = model(leaves)\n",
    "            recon_error = F.mse_loss(x_recon, leaves)\n",
    "            val_loss += recon_error.item() + vq_loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Reconstruction Quality for a Single Example\")\n",
    "model.eval()\n",
    "\n",
    "# Save the model state_dict\n",
    "model_path = \"C:/Users/zphrfx/Desktop/hdk/VQVDB/models/vqvae.pth\"\n",
    "\n",
    "# Visualize the reconstruction quality for a single example\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "\n",
    "# Get a random block from the dataset\n",
    "original_block = vdb_dataset[7312].unsqueeze(0).to(device)\n",
    "\n",
    "# Perform the full compression/decompression cycle\n",
    "indices = model.encode(original_block)\n",
    "reconstructed_block = model.decode(indices)\n",
    "\n",
    "# Detach from GPU and convert to numpy for plotting\n",
    "original_np = original_block.squeeze().cpu().numpy()\n",
    "reconstructed_np = reconstructed_block.squeeze().detach().cpu().numpy()\n",
    "error_np = np.abs(original_np - reconstructed_np)\n",
    "\n",
    "# Get consistent color limits for fair comparison\n",
    "vmin = min(original_np.min(), reconstructed_np.min())\n",
    "vmax = max(original_np.max(), reconstructed_np.max())\n",
    "\n",
    "# --- Plot 1: Slice-by-Slice Comparison ---\n",
    "fig, axes = plt.subplots(3, 3, figsize=(13, 10))\n",
    "slices_to_show = [1, 4, 7] # Show slices from the Z-axis of the 8x8x8 cube\n",
    "\n",
    "for i, slice_idx in enumerate(slices_to_show):\n",
    "    # Original\n",
    "    im1 = axes[i, 0].imshow(original_np[slice_idx, :, :], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    axes[i, 0].set_title(f'Original (Slice Z={slice_idx})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Reconstructed\n",
    "    im2 = axes[i, 1].imshow(reconstructed_np[slice_idx, :, :], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    axes[i, 1].set_title(f'Reconstructed (Slice Z={slice_idx})')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Error Map\n",
    "    im3 = axes[i, 2].imshow(error_np[slice_idx, :, :], cmap='magma')\n",
    "    axes[i, 2].set_title('Absolute Error')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "fig.colorbar(im1, ax=axes[:,:2], orientation='vertical', fraction=.1)\n",
    "fig.colorbar(im3, ax=axes[:,2], orientation='vertical', fraction=.1)\n",
    "plt.suptitle('Qualitative Reconstruction Analysis', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_np.flatten(), bins=50, alpha=0.7, label='Original')\n",
    "plt.hist(reconstructed_np.flatten(), bins=50, alpha=0.7, label='Reconstructed')\n",
    "plt.title('Histogram of Voxel Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import entropy\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence between two distributions.\"\"\"\n",
    "    p = p.flatten()\n",
    "    q = q.flatten()\n",
    "    p = p / np.sum(p)  # Normalize\n",
    "    q = q / np.sum(q)  # Normalize\n",
    "    return entropy(p, q)\n",
    "kl_div = kl_divergence(original_np, reconstructed_np)\n",
    "print(f\"KL Divergence between original and reconstructed blocks: {kl_div:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA of the learned codebook vectors:\")\n",
    "codebook = model.quantizer.embedding.data.cpu()\n",
    "pca = PCA(n_components=2)\n",
    "codebook_2d = pca.fit_transform(codebook)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(codebook_2d[:, 0], codebook_2d[:, 1], s=15, alpha=0.7)\n",
    "plt.title('VQ-VAE Codebook (PCA Projection)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Codebook Usage Histogram ---\n",
    "# This is a powerful diagnostic. It requires running the encoder on the whole dataset.\n",
    "print(\"\\nCalculating codebook usage across the entire dataset...\")\n",
    "model.eval()\n",
    "all_indices = []\n",
    "# Create a dataloader without shuffling to iterate through the dataset\n",
    "full_loader = DataLoader(vdb_dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_batch in full_loader:\n",
    "        data_batch = data_batch.to(device) # Move data to the same device as the model\n",
    "        indices = model.encode(data_batch)\n",
    "        all_indices.append(indices.cpu().numpy().flatten())\n",
    "\n",
    "all_indices = np.concatenate(all_indices)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_indices, bins=NUM_EMBEDDINGS, range=(0, NUM_EMBEDDINGS-1))\n",
    "plt.title('Codebook Usage Frequency')\n",
    "plt.xlabel('Codebook Index')\n",
    "plt.ylabel('Number of Times Used')\n",
    "plt.show()\n",
    "\n",
    "num_dead_codes = NUM_EMBEDDINGS - len(np.unique(all_indices))\n",
    "print(f\"Number of 'dead' (unused) codes: {num_dead_codes} out of {NUM_EMBEDDINGS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb13bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Codebook Perplexity + Active-Code Ratio ---\n",
    "counts = np.bincount(all_indices, minlength=NUM_EMBEDDINGS).astype(np.float64)\n",
    "probs = counts / counts.sum()\n",
    "nonzero = probs > 0\n",
    "perplexity = np.exp(-(probs[nonzero] * np.log(probs[nonzero])).sum())\n",
    "active_ratio = nonzero.mean()\n",
    "\n",
    "print(f\"Codebook perplexity: {perplexity:.2f}\")\n",
    "print(f\"Active-code ratio  : {active_ratio*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def psnr(x, y, vmax=1.0):\n",
    "    mse = torch.mean((x - y) ** 2).item()\n",
    "    return 20 * log10(vmax) - 10 * log10(mse + 1e-12)\n",
    "\n",
    "model.eval()\n",
    "psnr_list, mse_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(vdb_dataset_val, batch_size=BATCH_SIZE, shuffle=False):\n",
    "        batch = batch.to(device)\n",
    "        rec = model.decode(model.encode(batch))\n",
    "        mse = ((batch - rec) ** 2).view(len(batch), -1).mean(dim=1)\n",
    "        mse_list.extend(mse.cpu().numpy())\n",
    "        psnr_list.extend([psnr(b, r) for b, r in zip(batch, rec)])\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(psnr_list, bins=40, alpha=.8)\n",
    "plt.title('PSNR Distribution'); plt.xlabel('PSNR (dB)'); plt.ylabel('Blocks')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(mse_list, bins=40, alpha=.8, log=True)  # log-y to see the tail\n",
    "plt.title('MSE Distribution'); plt.xlabel('MSE'); plt.ylabel('Blocks (log scale)')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f19461",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100_000\n",
    "orig_sample = original_np.flatten()\n",
    "recon_sample = reconstructed_np.flatten()\n",
    "if len(orig_sample) > n_points:\n",
    "    idx = np.random.choice(len(orig_sample), n_points, replace=False)\n",
    "    orig_sample = orig_sample[idx]; recon_sample = recon_sample[idx]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(orig_sample, recon_sample, s=2, alpha=.5)\n",
    "lims = [min(orig_sample.min(), recon_sample.min()),\n",
    "        max(orig_sample.max(), recon_sample.max())]\n",
    "plt.plot(lims, lims, 'k--', linewidth=1)\n",
    "plt.xlabel('Original voxel'); plt.ylabel('Reconstructed voxel')\n",
    "plt.title('Voxel-wise Scatter (diag = perfect)')\n",
    "plt.grid(True, alpha=.3); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20837d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- L2 norm of each embedding vector ---\n",
    "embed_norm = torch.linalg.norm(model.quantizer.embedding.data, dim=1).cpu().numpy()\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.bar(range(NUM_EMBEDDINGS), embed_norm, width=1.0)\n",
    "plt.title('Codebook Embedding L2 Norms'); plt.xlabel('Code Index'); plt.ylabel('Norm')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f22a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mip(vol, axis):\n",
    "    \"\"\"Maximum-intensity projection along a single axis.\"\"\"\n",
    "    return vol.max(axis=axis)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 7))\n",
    "views = [(0, 'XY MIP'),   # collapse Z\n",
    "         (1, 'XZ MIP'),   # collapse Y\n",
    "         (2, 'YZ MIP')]   # collapse X\n",
    "\n",
    "for col, (axis_to_collapse, title) in enumerate(views):\n",
    "    axes[0, col].imshow(mip(original_np, axis=axis_to_collapse), cmap='viridis')\n",
    "    axes[0, col].set_title(f'Original {title}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(mip(reconstructed_np, axis=axis_to_collapse), cmap='viridis')\n",
    "    axes[1, col].set_title(f'Reconstructed {title}')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Maximum-Intensity Projections (3-view)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c617293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. Build a per-block latent vector ----------\n",
    "model.eval()\n",
    "latents, errs = [], []          # errs = optional colouring\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(vdb_dataset_val,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        idx = model.encode(batch).long()               # (B, Z, Y, X) indices\n",
    "        emb = model.quantizer.embedding[idx.view(-1)]  # (B*Z*Y*X, C)\n",
    "        emb = emb.view(*idx.shape, -1)                 # (B, Z, Y, X, C)\n",
    "        mean_emb = emb.mean(dim=(1, 2, 3))             # (B, C)\n",
    "        latents.append(mean_emb.cpu())\n",
    "        \n",
    "        # Optional: per-block MSE for coloured scatter\n",
    "        rec = model.decode(idx)\n",
    "        errs.append(((batch - rec) ** 2)\n",
    "                    .view(len(batch), -1)\n",
    "                    .mean(dim=1)\n",
    "                    .cpu())\n",
    "\n",
    "latents = torch.cat(latents, dim=0).numpy()   # (N, C)\n",
    "errs    = torch.cat(errs, dim=0).numpy()      # (N,)\n",
    "\n",
    "# ---------- 2. PCA to 2-D ----------\n",
    "from sklearn.decomposition import FastICA\n",
    "pca2 = FastICA(n_components=2, random_state=0)\n",
    "latents_2d = pca2.fit_transform(latents)      # (N, 2)\n",
    "\n",
    "# ---------- 3. Scatter with viridis ----------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sc = plt.scatter(latents_2d[:, 0],\n",
    "                 latents_2d[:, 1],\n",
    "                 c=errs,                 # <- set to None for uniform colour\n",
    "                 cmap='viridis',\n",
    "                 s=4,\n",
    "                 alpha=0.8)\n",
    "if sc.get_array() is not None:           # only if colouring by a value\n",
    "    plt.colorbar(sc, label='Block MSE')\n",
    "\n",
    "plt.title('Latent Space Sampling (PCA-2D, viridis)')\n",
    "plt.xlabel('PC-1'); plt.ylabel('PC-2')\n",
    "plt.grid(True, alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
